import gradio as gr
import os
import json
from typing import Optional, Tuple, List, Dict, Any
import warnings

# å¯¼å…¥å„ä¸ªæ¨¡å—
from indexing.indexer import Indexer
from indexing.verctorstore import VectorStore
from llm.llm import get_llm, ChatModel
from retrieval.retriever import get_similarity_retriever, BM25Retriever, FusionRetriever
from reranking.reranker import get_reranker
from query.query_translation import (
    get_rewritten_query_retriever, 
    get_multi_query_retriever, 
    get_rag_fusion_retriever,
    get_step_back_retriever
)
from routing.routing import semantic_routing
from utils.logging import setup_logging
from rank_bm25 import BM25Okapi
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

warnings.filterwarnings("ignore")

# é…ç½®æ—¥å¿—
logger = setup_logging(log_level="INFO")

class RAGApplication:
    def __init__(self):
        self.vectorstore: Optional[VectorStore] = None
        self.bm25_index: Optional[BM25Okapi] = None
        self.llm: Optional[ChatModel] = None
        self.retriever = None
        self.config = self._load_default_config()
        self.indexed_files: List[str] = []
        
    def _load_default_config(self) -> Dict[str, Any]:
        """åŠ è½½é»˜è®¤é…ç½®"""
        return {
            "llm": {
                "provider": "openai",
                "model": "gpt-3.5-turbo",
                "api_key": "",
                "api_base": "https://api.openai.com/v1",
                "model_config": {
                    "temperature": 0.7,
                    "max_tokens": 1000
                }
            },
            "embedding": {
                "model_name": "sentence-transformers/all-MiniLM-L6-v2",
                "model_kwargs": {}
            },
            "chunker": {
                "type": "recursive",
                "params": {
                    "chunk_size": 512,
                    "chunk_overlap": 64
                }
            },
            "vectorstore": {
                "persist_directory": "./vector_store"
            },
            "retrieval": {
                "type": "similarity",
                "k": 5,
                "use_reranker": False,
                "rerank_model": "rerank-english-v3.0",
                "rerank_top_n": 5
            },
            "query_enhancement": {
                "type": "none",  # none, rewrite, multi_query, rag_fusion, step_back
            },
            "routing": {
                "enabled": False,
                "prompt_templates": []
            }
        }
    
    def update_config(self, config_json: str) -> str:
        """æ›´æ–°é…ç½®"""
        try:
            new_config = json.loads(config_json)
            self.config.update(new_config)
            logger.info("é…ç½®æ›´æ–°æˆåŠŸ")
            return "âœ… é…ç½®æ›´æ–°æˆåŠŸ"
        except json.JSONDecodeError as e:
            error_msg = f"âŒ é…ç½®JSONæ ¼å¼é”™è¯¯: {str(e)}"
            logger.error(error_msg)
            return error_msg
        except Exception as e:
            error_msg = f"âŒ é…ç½®æ›´æ–°å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return error_msg
    
    def init_llm(self) -> str:
        """åˆå§‹åŒ–LLM"""
        try:
            self.llm = get_llm(self.config["llm"])
            logger.info(f"LLMåˆå§‹åŒ–æˆåŠŸ: {self.config['llm']['provider']}")
            return f"âœ… LLMåˆå§‹åŒ–æˆåŠŸ: {self.config['llm']['provider']} - {self.config['llm']['model']}"
        except Exception as e:
            error_msg = f"âŒ LLMåˆå§‹åŒ–å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return error_msg
    
    def index_documents(self, file_paths: List[str]) -> str:
        """ç´¢å¼•æ–‡æ¡£"""
        try:
            if not file_paths:
                return "âŒ è¯·é€‰æ‹©è¦ç´¢å¼•çš„æ–‡ä»¶"
            
            indexer = Indexer(self.config)
            
            for file_path in file_paths:
                if file_path in self.indexed_files:
                    continue
                    
                result = indexer.index(file_path)
                if result is None:
                    continue
                    
                vectorstore, bm25_index = result
                
                if self.vectorstore is None:
                    self.vectorstore = vectorstore
                    self.bm25_index = bm25_index
                else:
                    # åˆå¹¶ç´¢å¼•ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯èƒ½éœ€è¦æ›´å¤æ‚çš„åˆå¹¶é€»è¾‘ï¼‰
                    pass
                
                self.indexed_files.append(file_path)
                logger.info(f"æ–‡æ¡£ç´¢å¼•å®Œæˆ: {file_path}")
            
            return f"âœ… æ–‡æ¡£ç´¢å¼•å®Œæˆï¼Œå·²ç´¢å¼• {len(self.indexed_files)} ä¸ªæ–‡ä»¶"
            
        except Exception as e:
            error_msg = f"âŒ æ–‡æ¡£ç´¢å¼•å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return error_msg
    
    def _get_retriever(self):
        """æ ¹æ®é…ç½®è·å–æ£€ç´¢å™¨"""
        if self.vectorstore is None or self.bm25_index is None:
            raise ValueError("è¯·å…ˆç´¢å¼•æ–‡æ¡£")
        
        retrieval_config = self.config["retrieval"]
        retrieval_type = retrieval_config["type"]
        k = retrieval_config["k"]
        
        if retrieval_type == "similarity":
            base_retriever = get_similarity_retriever(self.vectorstore, k)
        elif retrieval_type == "bm25":
            base_retriever = BM25Retriever(self.bm25_index, k)
        elif retrieval_type == "fusion":
            base_retriever = FusionRetriever(self.vectorstore, self.bm25_index, k=k)
        else:
            raise ValueError(f"æœªçŸ¥çš„æ£€ç´¢ç±»å‹: {retrieval_type}")
        
        # åº”ç”¨é‡æ’åº
        if retrieval_config.get("use_reranker", False):
            try:
                retriever = get_reranker(
                    base_retriever,
                    retrieval_config.get("rerank_model", "rerank-english-v3.0"),
                    retrieval_config.get("rerank_top_n", 5)
                )
            except Exception as e:
                logger.warning(f"é‡æ’åºå™¨åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ£€ç´¢å™¨: {str(e)}")
                retriever = base_retriever
        else:
            retriever = base_retriever
        
        return retriever
    
    def _get_enhanced_retriever(self, base_retriever):
        """è·å–å¢å¼ºæ£€ç´¢å™¨"""
        query_config = self.config["query_enhancement"]
        enhancement_type = query_config["type"]
        
        if enhancement_type == "none" or self.llm is None:
            return base_retriever
        elif enhancement_type == "rewrite":
            return get_rewritten_query_retriever(base_retriever, self.llm)
        elif enhancement_type == "multi_query":
            return get_multi_query_retriever(base_retriever, self.llm)
        elif enhancement_type == "rag_fusion":
            return get_rag_fusion_retriever(base_retriever, self.llm)
        elif enhancement_type == "step_back":
            return get_step_back_retriever(base_retriever, self.llm)
        else:
            return base_retriever
    
    def query(self, question: str, chat_history: List[Tuple[str, str]]) -> Tuple[str, List[Tuple[str, str]]]:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢"""
        try:
            if not question.strip():
                return "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜", chat_history
            
            if self.llm is None:
                return "âŒ è¯·å…ˆåˆå§‹åŒ–LLM", chat_history
            
            if self.vectorstore is None:
                return "âŒ è¯·å…ˆç´¢å¼•æ–‡æ¡£", chat_history
            
            # è·å–æ£€ç´¢å™¨
            base_retriever = self._get_retriever()
            enhanced_retriever = self._get_enhanced_retriever(base_retriever)
            
            # è¯­ä¹‰è·¯ç”±ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.config["routing"]["enabled"]:
                try:
                    prompt_template = semantic_routing(question, self.config["routing"])
                except Exception as e:
                    logger.warning(f"è¯­ä¹‰è·¯ç”±å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿: {str(e)}")
                    prompt_template = ChatPromptTemplate.from_template(
                        "åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜:\n\n{context}\n\né—®é¢˜: {question}\n\nå›ç­”:"
                    )
            else:
                prompt_template = ChatPromptTemplate.from_template(
                    "åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜:\n\n{context}\n\né—®é¢˜: {question}\n\nå›ç­”:"
                )
            
            # æ„å»ºRAGé“¾
            def format_docs(docs):
                return "\n\n".join(doc.page_content for doc in docs)
            
            rag_chain = (
                {"context": enhanced_retriever | format_docs, "question": RunnablePassthrough()}
                | prompt_template
                | self.llm
                | StrOutputParser()
            )
            
            # æ‰§è¡ŒæŸ¥è¯¢
            response = rag_chain.invoke(question)
            
            # æ›´æ–°èŠå¤©å†å²
            chat_history.append((question, response))
            
            logger.info(f"æŸ¥è¯¢å¤„ç†å®Œæˆ: {question[:50]}...")
            return response, chat_history
            
        except Exception as e:
            error_msg = f"âŒ æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            chat_history.append((question, error_msg))
            return error_msg, chat_history
    
    def get_current_config(self) -> str:
        """è·å–å½“å‰é…ç½®"""
        return json.dumps(self.config, indent=2, ensure_ascii=False)
    
    def clear_index(self) -> str:
        """æ¸…ç©ºç´¢å¼•"""
        self.vectorstore = None
        self.bm25_index = None
        self.indexed_files.clear()
        logger.info("ç´¢å¼•å·²æ¸…ç©º")
        return "âœ… ç´¢å¼•å·²æ¸…ç©º"

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    app = RAGApplication()
    
    with gr.Blocks(title="RAGåº”ç”¨ç³»ç»Ÿ", theme=gr.themes.Soft()) as interface:
        gr.Markdown("# ğŸ¤– RAGåº”ç”¨ç³»ç»Ÿ")
        gr.Markdown("ä¸€ä¸ªåŠŸèƒ½å®Œæ•´çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿï¼Œæ”¯æŒå¤šç§RAGæŠ€æœ¯çš„é…ç½®å’Œä½¿ç”¨")
        
        with gr.Tabs():
            # é…ç½®æ ‡ç­¾é¡µ
            with gr.Tab("âš™ï¸ ç³»ç»Ÿé…ç½®"):
                gr.Markdown("### LLMé…ç½®")
                with gr.Row():
                    with gr.Column():
                        config_input = gr.Code(
                            value=app.get_current_config(),
                            language="json",
                            label="é…ç½®JSON",
                            lines=20
                        )
                        update_config_btn = gr.Button("æ›´æ–°é…ç½®", variant="primary")
                        config_status = gr.Textbox(label="é…ç½®çŠ¶æ€", interactive=False)
                    
                    with gr.Column():
                        init_llm_btn = gr.Button("åˆå§‹åŒ–LLM", variant="primary")
                        llm_status = gr.Textbox(label="LLMçŠ¶æ€", interactive=False)
                        
                        gr.Markdown("### å½“å‰é…ç½®è¯´æ˜")
                        gr.Markdown("""
                        **æ”¯æŒçš„é…ç½®é€‰é¡¹ï¼š**
                        - **LLM**: OpenAI, Groq
                        - **åˆ†å—å™¨**: recursive, token, SemanticSpacyChunker, SemanticNLTKChunker
                        - **æ£€ç´¢ç±»å‹**: similarity, bm25, fusion
                        - **æŸ¥è¯¢å¢å¼º**: none, rewrite, multi_query, rag_fusion, step_back
                        - **é‡æ’åº**: æ”¯æŒCohere Rerank
                        """)
            
            # æ–‡æ¡£ç®¡ç†æ ‡ç­¾é¡µ
            with gr.Tab("ğŸ“ æ–‡æ¡£ç®¡ç†"):
                with gr.Row():
                    with gr.Column():
                        file_input = gr.File(
                            label="é€‰æ‹©æ–‡æ¡£æ–‡ä»¶",
                            file_count="multiple",
                            file_types=[".pdf"]
                        )
                        index_btn = gr.Button("ç´¢å¼•æ–‡æ¡£", variant="primary")
                        clear_btn = gr.Button("æ¸…ç©ºç´¢å¼•", variant="secondary")
                    
                    with gr.Column():
                        index_status = gr.Textbox(label="ç´¢å¼•çŠ¶æ€", interactive=False)
                        indexed_files = gr.Textbox(
                            label="å·²ç´¢å¼•æ–‡ä»¶",
                            value="æš‚æ— ",
                            interactive=False,
                            lines=5
                        )
            
            # å¯¹è¯æ ‡ç­¾é¡µ
            with gr.Tab("ğŸ’¬ æ™ºèƒ½å¯¹è¯"):
                chatbot = gr.Chatbot(
                    label="å¯¹è¯å†å²",
                    height=400,
                    bubble_full_width=False
                )
                
                with gr.Row():
                    question_input = gr.Textbox(
                        label="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜",
                        placeholder="ä¾‹å¦‚ï¼šè¯·ä»‹ç»ä¸€ä¸‹æ–‡æ¡£ä¸­çš„ä¸»è¦å†…å®¹...",
                        scale=4
                    )
                    submit_btn = gr.Button("å‘é€", variant="primary", scale=1)
                
                clear_chat_btn = gr.Button("æ¸…ç©ºå¯¹è¯", variant="secondary")
        
        # äº‹ä»¶ç»‘å®š
        update_config_btn.click(
            app.update_config,
            inputs=[config_input],
            outputs=[config_status]
        )
        
        init_llm_btn.click(
            app.init_llm,
            outputs=[llm_status]
        )
        
        index_btn.click(
            app.index_documents,
            inputs=[file_input],
            outputs=[index_status]
        ).then(
            lambda: "\n".join(app.indexed_files) if app.indexed_files else "æš‚æ— ",
            outputs=[indexed_files]
        )
        
        clear_btn.click(
            app.clear_index,
            outputs=[index_status]
        ).then(
            lambda: "æš‚æ— ",
            outputs=[indexed_files]
        )
        
        submit_btn.click(
            app.query,
            inputs=[question_input, chatbot],
            outputs=[question_input, chatbot]
        ).then(
            lambda: "",
            outputs=[question_input]
        )
        
        question_input.submit(
            app.query,
            inputs=[question_input, chatbot],
            outputs=[question_input, chatbot]
        ).then(
            lambda: "",
            outputs=[question_input]
        )
        
        clear_chat_btn.click(
            lambda: [],
            outputs=[chatbot]
        )
    
    return interface

def main():
    """ä¸»å‡½æ•°"""
    logger.info("å¯åŠ¨RAGåº”ç”¨ç³»ç»Ÿ")
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs("./vector_store", exist_ok=True)
    os.makedirs("./logs", exist_ok=True)
    
    # åˆ›å»ºå¹¶å¯åŠ¨ç•Œé¢
    interface = create_interface()
    interface.launch(
        share=False,
        inbrowser=True,
        server_name="0.0.0.0",
        server_port=7860
    )

if __name__ == "__main__":
    main()
